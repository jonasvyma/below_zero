{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a5da1a",
   "metadata": {},
   "source": [
    "# Extracting `DAILY` data from API and Loading it to DB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee8342",
   "metadata": {},
   "source": [
    "As the purpose of our pipeline is to make Weather Data available for comparison to flights and airports data, in the first step we need to load the weather data in a raw form (JSON) into our database. So in later steps we can transform it to meaningful and useful tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6b31b",
   "metadata": {},
   "source": [
    "### General Presteps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff19378",
   "metadata": {},
   "source": [
    "The Goal of this Notebook is to get raw JSON data for Daily and Hourly Weather for 3 airport weather stations and load it as it is to our database.\n",
    "- Find Station IDs for **defined** airports \n",
    "- Define the start and end of the period\n",
    "- get the API Key from the `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8dd7e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a362233",
   "metadata": {},
   "source": [
    "we will need the credentials we saved in the `.env` file. We also will need SQLAlchemy and its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9602ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need the credentials we saved in the .env file\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# We also will need SQLAlchemy and its functions\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy.dialects.postgresql import JSON as postgres_json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# requests library will make the API calls. \n",
    "# the json package will parse the JSON string and convert it to Python data structures\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# with 'datetime' we want to catch the timestamp of the API call. For the actuality reference. \n",
    "# and 'time' for slowing down a .bit\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f4304",
   "metadata": {},
   "source": [
    "### Defining Airports andd finding the Station IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead01412",
   "metadata": {},
   "source": [
    "For our Pipeline we will use weather data from the weather stations at the 3 highly frequented airports\n",
    "- **JFK**: John F. Kennedy Airport\n",
    "- **MIA**: Miami International Airport\n",
    "- **LAX**: Los Angeles Airport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19c4cd",
   "metadata": {},
   "source": [
    "To find the Station IDs for the airpors without stressing our API Call limits, we will use the   search option of the **https://meteostat.net/**  \n",
    "\n",
    "We can search for the names of the airports above and find the Station IDs.\n",
    "\n",
    "Let's add them to the dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_staids = {\n",
    "    'JFK': ???\n",
    "    ,'MIA': ???\n",
    "    ,'LAX': ???\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee31247",
   "metadata": {},
   "source": [
    "### Defining the period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abdd4f",
   "metadata": {},
   "source": [
    "Our flight Data is from 2024-01-01 until 2024-03-31. For the lectures we will use the same period for the meteostat JSON API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_start = \"2024-01-01\"\n",
    "period_end = \"2024-03-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56ded0",
   "metadata": {},
   "source": [
    "### loading API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting API and DB credentials - Alternative 1: dotenv_values()\n",
    "\n",
    "config = dotenv_values()\n",
    "\n",
    "api_key = config['?????????'] # align the key label with your .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1b051",
   "metadata": {},
   "source": [
    "# Part 1: Daily Station Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b016",
   "metadata": {},
   "source": [
    "Each API call will get 3 months of weather data for one Station ID.  \n",
    "\n",
    "In the [**RapidAPI**](https://rapidapi.com/meteostat/api/meteostat/playground) interface you can find the code syntax we need to make the call. \n",
    "\n",
    "For each call we need to create a querystring with required parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e2d7d",
   "metadata": {},
   "source": [
    "### Objectives -  Daily Station Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c14df",
   "metadata": {},
   "source": [
    "- create a for-loop for the 3 airports, generating a **querystring for each API call**\n",
    "- define an empty dictionary to collect: \n",
    "  - time of the call\n",
    "  - airport code\n",
    "  - station id\n",
    "  - related data\n",
    "- make the API calls using the for-loop and fill the dictionary\n",
    "- create pandas dataframe from the dictionary\n",
    "- load the DB credentials from the `.env`\n",
    "- create the engine\n",
    "- define data types for the postgresql table columns\n",
    "- using pandas import the dataframe to the Table in the Schema of the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4c2aa",
   "metadata": {},
   "source": [
    "### Test: For-loop generating the querystrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for-loop: querystring for each airport\n",
    "\n",
    "for airport in airport_staids:\n",
    "   \n",
    "    querystring = {\n",
    "        \"station\": ??? # corresponding value in 'airport_staids' dictionary\n",
    "        ,\"start\": ??? # variable we used to define a period start\n",
    "        ,\"end\": ??? # variable we used to define a period end\n",
    "        ,\"model\":\"true\" # what does this parameter do? check meteostat documentation.\n",
    "    }\n",
    "    print(airport, \"\\n\", querystring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d989d3",
   "metadata": {},
   "source": [
    "### API CALL daily (per station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291829d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let's catch each response in a dictionary. create an empty dictionary with the following keys:\n",
    "\n",
    "weather_dict = {'extracted_at':[], \n",
    "                'airport_code':[], \n",
    "                'station_id':[], \n",
    "                'extracted_data':[]\n",
    "               }\n",
    "\n",
    "# API CALL daily (station) - for the syntax: see the rapidapi interface\n",
    "\n",
    "url = \"https://meteostat.p.rapidapi.com/stations/daily\"\n",
    "\n",
    "headers = {\n",
    "        \"X-RapidAPI-Key\": api_key,\n",
    "        \"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# for-loop for the querystrings\n",
    "for airport in airport_staids:\n",
    "   \n",
    "    querystring = {\n",
    "        \"station\":airport_staids[airport]\n",
    "        ,\"start\":period_start\n",
    "        ,\"end\":period_end\n",
    "        ,\"model\":\"true\"\n",
    "    }\n",
    "    \n",
    "    # making one call with the current querystring\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "                \n",
    "    # appending data to the dictionary:\n",
    "    weather_dict['extracted_at'].append(???)       # timestamp, \n",
    "    weather_dict['airport_code'].append(???)       # airport code    \n",
    "    weather_dict['station_id'].append(???)         # weater Station ID\n",
    "    weather_dict['extracted_data'].append(json.loads(response.text))   # JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3849a2",
   "metadata": {},
   "source": [
    "#### Checkout the filled dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f23980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308b9cf",
   "metadata": {},
   "source": [
    "### Make it a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9180ca",
   "metadata": {},
   "source": [
    "this is our raw data, which we now can load into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f534511",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_daily_df = pd.DataFrame(weather_dict)\n",
    "weather_daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe6265",
   "metadata": {},
   "source": [
    "### SIDEBAR: For the curious and sceptics..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0b84",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "    In case you can't resist to know what the data looks like when flattened. \n",
    "    Here is the preview with pandas. BUT we are not transforming before loading in our pipeline just yet. \n",
    "    We Extract and Load the raw JSON."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e714f16",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# using pd.json_normalize() twice to get to the weather_stats of one airport under 'data'\n",
    "\n",
    "df_JFK = pd.json_normalize(pd.json_normalize(weather_daily_df['extracted_data']).loc[0, 'data'])\n",
    "df_JFK"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c12d47aa",
   "metadata": {},
   "source": [
    "#  compare if needed to the JSON for 'JFK', first row \n",
    "weather_daily_df.loc[0,'extracted_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbc530",
   "metadata": {},
   "source": [
    "> #### Note: we only used up 3 API calls per attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7702d",
   "metadata": {},
   "source": [
    "### Loading the data into the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0d541",
   "metadata": {},
   "source": [
    "Now all we need to create a table in your Schema in our database is part of the `weather_daily_df` dataframe.  \n",
    "We can use pandas' ability to work with SQLAlchemy and \"save\" the data to the DB using the `.to_sql()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f61101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting API and DB credentials - Alternative 1: dotenv_values()\n",
    "\n",
    "config = dotenv_values()\n",
    " \n",
    "pg_user = config['POSTGRES_USER'] # align the key labels with your .env file\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_pass = config['POSTGRES_PASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46208c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the url\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "\n",
    "# creating the engine\n",
    "engine = create_engine(url, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d21cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.url # checking the url (pass is hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining data types for the DB\n",
    "dtype_dict = {\n",
    "    'extracted_at':types.DateTime,\n",
    "    'airport_code': types.String,\n",
    "    'station_id': types.Integer,\n",
    "    'extracted_data':postgres_json\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa287d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing dataframe to DB\n",
    "weather_daily_df.to_sql(name = 'weather_daily_raw', \n",
    "                       con = engine, \n",
    "                       schema = pg_schema, # pandas is allowing to specify, in which schema the table shall be created\n",
    "                       if_exists='replace', \n",
    "                       dtype=dtype_dict,\n",
    "                       index=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507453c",
   "metadata": {},
   "source": [
    "The result of the last cell should give you the number of rows you imported to the DB table `weather_daily_raw`. Each row contains data from one API call.\n",
    "\n",
    "Check in DBeaver if you see a new table in your Schema. Don't forget to refresh your Schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017fd10",
   "metadata": {},
   "source": [
    "## Now continue with the hourly data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
